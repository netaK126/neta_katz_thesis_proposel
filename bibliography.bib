@inproceedings{roelofs2019meta,
  title={A meta-analysis of overfitting in machine learning},
  author={Roelofs, Rebecca and Fridovich-Keil, Sara and Miller, John and Shankar, Vaishaal and Hardt, Moritz and Recht, Benjamin and Schmidt, Ludwig},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={9179--9189},
  year={2019}
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization. arXiv preprint (In ICLR 2017)},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}

@inproceedings{garg2021ratt,
  title={Ratt: Leveraging unlabeled data to guarantee generalization},
  author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary},
  booktitle={International Conference on Machine Learning},
  pages={3598--3609},
  year={2021},
  organization={PMLR}
}

@incollection{hoeffding1994probability,
  title={Probability inequalities for sums of bounded random variables},
  author={Hoeffding, Wassily},
  booktitle={The Collected Works of Wassily Hoeffding},
  pages={409--426},
  year={1994},
  publisher={Springer}
}

@article{bardenet2015concentration,
  title={Concentration inequalities for sampling without replacement},
  author={Bardenet, R{\'e}mi and Maillard, Odalric-Ambrym and others},
  journal={Bernoulli},
  volume={21},
  number={3},
  pages={1361--1385},
  year={2015},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{OnePixelAttack,
   title={One Pixel Attack for Fooling Deep Neural Networks},
   volume={23},
   ISSN={1941-0026},
   url={http://dx.doi.org/10.1109/TEVC.2019.2890858},
   DOI={10.1109/tevc.2019.2890858},
   number={5},
   journal={IEEE Transactions on Evolutionary Computation},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
   year={2019},
   month={Oct},
   pages={828–841} }

@misc{szegedy2014intriguing,
      title={Intriguing properties of neural networks},
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{taxonomy,
author = {Wu, Junde and Fu, Rao},
year = {2019},
month = {08},
pages = {},
title = {Universal, transferable and targeted adversarial attacks}
}

@misc{he2015deep,
      title={Deep Residual Learning for Image Recognition},
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@incollection{GOLDBERG199169,
title = {A Comparative Analysis of Selection Schemes Used in Genetic Algorithms},
editor = {GREGORY J.E. RAWLINS},
series = {Foundations of Genetic Algorithms},
publisher = {Elsevier},
volume = {1},
pages = {69-93},
year = {1991},
issn = {1081-6593},
doi = {https://doi.org/10.1016/B978-0-08-050684-5.50008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780080506845500082},
author = {David E. Goldberg and Kalyanmoy Deb}
}

@misc{real2019regularized,
      title={Regularized Evolution for Image Classifier Architecture Search},
      author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
      year={2019},
      eprint={1802.01548},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{alatalo2021chromatic,
      title={Chromatic and spatial analysis of one-pixel attacks against an image classifier},
      author={Janne Alatalo and Joni Korpihalkola and Tuomo Sipola and Tero Kokkonen},
      year={2021},
      eprint={2105.13771},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vargas2019understanding,
      title={Understanding the One-Pixel Attack: Propagation Maps and Locality Analysis},
      author={Danilo Vasconcellos Vargas and Jiawei Su},
      year={2019},
      eprint={1902.02947},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{krizhevsky2014weird,
      title={One weird trick for parallelizing convolutional neural networks},
      author={Alex Krizhevsky},
      year={2014},
      eprint={1404.5997},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need},
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ribeiro2016why,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples},
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{papernot2015limitations,
      title={The Limitations of Deep Learning in Adversarial Settings},
      author={Nicolas Papernot and Patrick McDaniel and Somesh Jha and Matt Fredrikson and Z. Berkay Celik and Ananthram Swami},
      year={2015},
      eprint={1511.07528},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}

@misc{moosavidezfooli2016deepfool,
      title={DeepFool: a simple and accurate method to fool deep neural networks},
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Pascal Frossard},
      year={2016},
      eprint={1511.04599},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{BlackBox1,
  author    = {Nicolas Papernot and
               Patrick D. McDaniel and
               Ian J. Goodfellow and
               Somesh Jha and
               Z. Berkay Celik and
               Ananthram Swami},
  title     = {Practical Black-Box Attacks against Deep Learning Systems using Adversarial
               Examples},
  journal   = {CoRR},
  volume    = {abs/1602.02697},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.02697},
  eprinttype = {arXiv},
  eprint    = {1602.02697},
  timestamp = {Mon, 13 Aug 2018 16:49:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PapernotMGJCS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{BlackBox2,
  author    = {Hao Qiu and
               Leonardo Lucio Custode and
               Giovanni Iacca},
  title     = {Black-box adversarial attacks using Evolution Strategies},
  journal   = {CoRR},
  volume    = {abs/2104.15064},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.15064},
  eprinttype = {arXiv},
  eprint    = {2104.15064},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-15064.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BlackBox3,
  author    = {Andrew Ilyas and
               Logan Engstrom and
               Anish Athalye and
               Jessy Lin},
  title     = {Black-box Adversarial Attacks with Limited Queries and Information},
  journal   = {CoRR},
  volume    = {abs/1804.08598},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.08598},
  eprinttype = {arXiv},
  eprint    = {1804.08598},
  timestamp = {Mon, 13 Aug 2018 16:47:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-08598.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BlackBox4,
  author    = {Laurent Meunier and
               Jamal Atif and
               Olivier Teytaud},
  title     = {Yet another but more efficient black-box adversarial attack: tiling
               and evolution strategies},
  journal   = {CoRR},
  volume    = {abs/1910.02244},
  year      = {2019},
  url       = {http://arxiv.org/abs/1910.02244},
  eprinttype = {arXiv},
  eprint    = {1910.02244},
  timestamp = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1910-02244.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{moosavidezfooli2017universal,
      title={Universal adversarial perturbations},
      author={Seyed-Mohsen Moosavi-Dezfooli and Alhussein Fawzi and Omar Fawzi and Pascal Frossard},
      year={2017},
      eprint={1610.08401},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{khrulkov2017art,
      title={Art of singular vectors and universal adversarial perturbations},
      author={Valentin Khrulkov and Ivan Oseledets},
      year={2017},
      eprint={1709.03582},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{mopuri2018nag,
      title={NAG: Network for Adversary Generation},
      author={Konda Reddy Mopuri and Utkarsh Ojha and Utsav Garg and R. Venkatesh Babu},
      year={2018},
      eprint={1712.03390},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inbook{inbook,
author = {Gulwani, S.},
year = {2016},
month = {04},
pages = {137-158},
title = {Programming by examples (and its applications in data wrangling)},
doi = {10.3233/978-1-61499-627-9-137}
}

@inproceedings{d39antoni2016qlose,
author = {D'Antoni, Loris and Samanta, Roopsha and Singh, Rishabh},
title = {Qlose: Program Repair with Quantiative Objectives},
booktitle = {27th International Conference on Computer Aided Verification (CAV 2016)},
year = {2016},
month = {July},
}

@article{10.1145/2666356.2594321,
author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
title = {Code Completion with Statistical Language Models},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594321},
doi = {10.1145/2666356.2594321},
abstract = {We address the problem of synthesizing code completions for programs using APIs. Given a program with holes, we synthesize completions for holes with the most likely sequences of method calls.Our main idea is to reduce the problem of code completion to a natural-language processing problem of predicting probabilities of sentences. We design a simple and scalable static analysis that extracts sequences of method calls from a large codebase, and index these into a statistical language model. We then employ the language model to find the highest ranked sentences, and use them to synthesize a code completion. Our approach is able to synthesize sequences of calls across multiple objects together with their arguments.Experiments show that our approach is fast and effective. Virtually all computed completions typecheck, and the desired completion appears in the top 3 results in 90% of the cases.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {419–428},
numpages = {10}
}

@inproceedings{10.1145/2594291.2594321,
author = {Raychev, Veselin and Vechev, Martin and Yahav, Eran},
title = {Code Completion with Statistical Language Models},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594321},
doi = {10.1145/2594291.2594321},
pages = {419–428},
numpages = {10},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@inproceedings{nori2015efficient,
author = {Nori, Aditya and Ozair, Sherjil and Rajamani, Sriram and Vijaykeerthy,  Deepak and },
title = {Efficient Synthesis of Probabilistic Programs},
booktitle = {Programming Language Design and Implementation (PLDI)},
year = {2015},
month = {June},
publisher = {ACM - Association for Computing Machinery},
url = {https://www.microsoft.com/en-us/research/publication/efficient-synthesis-of-probabilistic-programs/},
edition = {Programming Language Design and Implementation (PLDI)},
}

@article{10.1145/36205.36194,
author = {Massalin, Henry},
title = {Superoptimizer: A Look at the Smallest Program},
year = {1987},
issue_date = {Oct. 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {22},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/36205.36194},
doi = {10.1145/36205.36194},
abstract = {Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {122–126},
numpages = {5}
}

@inproceedings{10.1145/36206.36194,
author = {Massalin, Henry},
title = {Superoptimizer: A Look at the Smallest Program},
year = {1987},
isbn = {0818608056},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
url = {https://doi.org/10.1145/36206.36194},
doi = {10.1145/36206.36194},
pages = {122–126},
numpages = {5},
location = {Palo Alto, California, USA},
series = {ASPLOS II}
}

@INPROCEEDINGS{6679385,
  author={Alur, Rajeev and Bodik, Rastislav and Juniwal, Garvit and Martin, Milo M. K. and Raghothaman, Mukund and Seshia, Sanjit A. and Singh, Rishabh and Solar-Lezama, Armando and Torlak, Emina and Udupa, Abhishek},
  booktitle={2013 Formal Methods in Computer-Aided Design},
  title={Syntax-guided synthesis},
  year={2013},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/FMCAD.2013.6679385}}

@article{CSP,
author = {Ghédira, K. and Dubuisson, B.},
year = {2013},
month = {02},
pages = {},
title = {Constraint Satisfaction Problems: CSP Formalisms and Techniques},
journal = {Constraint Satisfaction Problems: CSP Formalisms and Techniques},
doi = {10.1002/9781118574522}
}

@misc{schkufza2012stochastic,
      title={Stochastic Superoptimization},
      author={Eric Schkufza and Rahul Sharma and Alex Aiken},
      year={2012},
      eprint={1211.0557},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}

@misc{parisotto2016neurosymbolic,
      title={Neuro-Symbolic Program Synthesis},
      author={Emilio Parisotto and Abdel-rahman Mohamed and Rishabh Singh and Lihong Li and Dengyong Zhou and Pushmeet Kohli},
      year={2016},
      eprint={1611.01855},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@book{Koza92,
  added-at = {2008-11-22T15:57:31.000+0100},
  address = {Cambridge, MA, USA},
  author = {Koza, John R.},
  biburl = {https://www.bibsonomy.org/bibtex/27573e564bc5369e1a853e74b3ac62607/emanuel},
  interhash = {e8307fb6cf4ee27405142256d98c4c9e},
  intrahash = {7573e564bc5369e1a853e74b3ac62607},
  isbn = {0-262-11170-5},
  keywords = {enumerative_ip gp induction inductive_programming program_evolution program_synthesis},
  publisher = {MIT Press},
  timestamp = {2008-11-22T15:57:31.000+0100},
  title = {Genetic Programming: {O}n the Programming of Computers by Means of Natural Selection},
  year = 1992
}


@article{ref7,
  title={Explaining and Harnessing Adversarial Examples.},
  author={Ian J. Goodfellow and
               Jonathon Shlens and
               Christian Szegedy.},
  year={2015},
  journal={In ICLR}
}



@article{ref15,
  title={Adversarial examples in the physical world.},
  author={Alexey Kurakin and
               Ian J. Goodfellow and
               Samy Bengio.},
  year={2017},
  journal={In ICLR }
}

@article{ref17,
  title={Exploring the Space of Adversarial Images.},
  author={Pedro Tabacof and
               Eduardo Valle.},
  year={2016},
  journal={In IJCNN}
}

@article{ref29,
  title={Adversarial Examples: Attacks and Defenses for Deep Learning.},
  author={Xiaoyong Yuan and
               Pan He and
               Qile Zhu and
               Xiaolin Li},
  year={2019},
  journal={In {IEEE} Trans. Neural Networks Learn. Syst}
}

@article{ref56,
 title={Towards Deep Learning Models Resistant to Adversarial Attacks},
 author={Aleksander Madry and
               Aleksandar Makelov and
               Ludwig Schmidt and
               Dimitris Tsipras and
               Adrian Vladu},
 journal={In ICLR},
 year={2018}
}

@misc{VANILLAGRADIENT,
title = {Deep inside convolutional networks: Visualising image classification models and saliency maps.},
author = {Simonyan, Karen, Andrea Vedaldi, and Andrew Zisserman},
year={2013},
eprint={1312.6034},
archivePrefix={arXiv}
}

@article{MIPVERIFY,
 title={Evaluating Robustness of Neural Networks with Mixed Integer Programming},
 author={Vincent Tjeng, Kai Xiao, Russ Tedrake},
 journal={ICLR},
 year={2019}
}

@article{MARVEL,
 title={Maximal Robust Neural Network Specifications via Oracle-guided Numerical Optimization},
 author={Anan Kabaha and Dana Drachsler Cohen},
 journal={VMCAI},
 year={2023}
}

@article{PGD,
title={Towards Deep Learning Models Resistant to Adversarial Attacks},
author = {Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, Adrian Vladu},
journal={ICLR},
year={2018}
}

@article{ABSTRACTINTER,
title={An abstract domain for certifying neural networks},
author = {Gagandeep Singh, Timon Gehr, Markus Püschel, Martin Vechev},
journal={ACM},
year={2019}
}

@misc{COMPLETE,
title={Fast and Complete: Enabling Complete Neural Network Verification with Rapid and Massively Parallel Incomplete Verifiers},
author = {Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, Cho-Jui Hsieh},
journal={ICLR},
year={2020},
month={11},
eprint={2011.13824},
archivePrefix={arXiv}
}

@article{INCOMPLETE1,
title={Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
author = {Timon Gehr; Matthew Mirman; Dana Drachsler-Cohen; Petar Tsankov; Swarat Chaudhuri; Martin Vechev},
journal={IEEE},
year={2018}
}

@article{INCOMPLETE2,
title={Fast and Effective Robustness Certification},
author = {Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Püschel, Martin Vechev},
journal={NeurIPS},
year={2018}
}

@article{SINGLElABEL1,
title={Threat of adversarial attacks on deep learning in computer vision: A survey},
author = {Naveed Akhtar and Ajmal Mian},
journal={IEEE},
year={2018}
}

@article{SINGLElABEL2,
title={Towards evaluating the robustness of neural networks},
author = {Nicholas Carlini and David Wagner},
journal={IEEE},
year={2017}
}

@article{SINGLElABEL3,
title={Explaining and harnessing adversarial examples},
author = {Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy},
journal={International Conference on Learning Representations},
year={2015}
}

@article{SINGLElABEL4,
title={Deepfool: a simple and accurate method to fool deep neural networks},
author = {Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard},
journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
year={2016}
}

@article{IMAGETAGGING,
title={Fast Image Tagging},
author={Minmin Chen, Alice Zheng, Kilian Weinberger},
    journal={Proceedings of the 30th International Conference on Machine Learning},
    year={2013},
}

@article{ObjectDetection,
title={A Trainable System for Object Detection},
author={Papageorgiou, C., Poggio, T},
    journal={International Journal of Computer Vision},
    year={2000},
}

@article{FacialRec,
title={Multiple Transfer Learning and Multi-Label Balanced Training Strategies for Facial AU Detection in the Wild},
author={Sijie Ji, Kai Wang, Xiaojiang Peng, Jianfei Yang, Zhaoyang Zeng, Yu Qiao},
    journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
    year={2020},
}

@misc{MultiVul1,
title={Adversarial Extreme Multi-label Classification},
author={Rohit Babbar, Bernhard Schölkopf},
    year={2018},
eprint={1803.01570},
archivePrefix={arXiv}
}

@article{MultiVul2,
title={MultiGuard: Provably Robust Multi-label Classification against Adversarial Examples},
author={Jinyuan Jia, Wenjie Qu, Neil Gong},
    year={2022},
journal={Advances in Neural Information Processing Systems}
}

@article{MultiVul3,
title={Domain Knowledge Alleviates Adversarial Attacks in Multi-Label Classifiers},
author={Stefano Melacci, Gabriele Ciravegna, Angelo Sotgiu, Ambra Demontis, Battista Biggio, Marco Gori},
    year={2022},
journal={IEEE}
}

@article{OracleGuided,
title={Oracle-guided component-based program synthesis},
author={Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.},
    year={2010},
journal={ICSE}
}

@article{DoubleMNIST,
title={Dynamic Routing Between Capsules},
author={Jha, S., Gulwani, S., Seshia, S.A., Tiwari, A.Sara Sabour, Nicholas Frosst, Geoffrey E. Hinton},
    year={2017},
journal={NeurIPS}
}

@article{L0,
title={Adversarial Training Against Location-Optimized Adversarial Patches},
author={Sukrut Rao, David Stutz, Bernt Schiele},
    year={2020},
journal={ECCV}
}

@ARTICLE{CEGIS1,
  author={Li, Changjiang and Ji, Shouling and Weng, Haiqin and Li, Bo and Shi, Jie and Beyah, Raheem and Guo, Shanqing and Wang, Zonghui and Wang, Ting},
  journal={IEEE Transactions on Dependable and Secure Computing},
  title={Towards Certifying the Asymmetric Robustness for Neural Networks: Quantification and Applications},
  year={2022},
  volume={19},
  number={6},
  pages={3987-4001},
  doi={10.1109/TDSC.2021.3116105}}

@InProceedings{CEGIS2,
author="Kabaha, Anan
and Drachsler-Cohen, Dana",
editor="Singh, Gagandeep
and Urban, Caterina",
title="Boosting Robustness Verification of Semantic Feature Neighborhoods",
booktitle="Static Analysis",
year="2022",
publisher="Springer Nature Switzerland",
address="Cham",
pages="299--324",
abstract="Deep neural networks have been shown to be vulnerable to adversarial attacks that perturb inputs based on semantic features. Existing robustness analyzers can reason about semantic feature neighborhoods to increase the networks' reliability. However, despite the significant progress in these techniques, they still struggle to scale to deep networks and large neighborhoods. In this work, we introduce VeeP, an active learning approach that splits the verification process into a series of smaller verification steps, each is submitted to an existing robustness analyzer. The key idea is to build on prior steps to predict the next optimal step. The optimal step is predicted by estimating the robustness analyzer's velocity and sensitivity via parametric regression. We evaluate VeeP on MNIST, Fashion-MNIST, CIFAR-10 and ImageNet and show that it can analyze neighborhoods of various features: brightness, contrast, hue, saturation, and lightness. We show that, on average, given a 90 minute timeout, VeeP verifies 96{\%} of the maximally certifiable neighborhoods within 29 minutes, while existing splitting approaches verify, on average, 73{\%} of the maximally certifiable neighborhoods within 58 minutes.",
isbn="978-3-031-22308-2"
}


@InProceedings{CEGIS3,
  title = 	 {On Certifying Non-Uniform Bounds against Adversarial Attacks},
  author =       {Liu, Chen and Tomioka, Ryota and Cevher, Volkan},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4072--4081},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/liu19h/liu19h.pdf},
  url = 	 {https://proceedings.mlr.press/v97/liu19h.html},
  abstract = 	 {This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.}
}

@misc{qin2019verification,
      title={Verification of Non-Linear Specifications for Neural Networks},
      author={Chongli Qin and Krishnamurthy and Dvijotham and Brendan O'Donoghue and Rudy Bunel and Robert Stanforth and Sven Gowal and Jonathan Uesato and Grzegorz Swirszcz and Pushmeet Kohli},
      year={2019},
      eprint={1902.09592},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{overapprox,
author = {Anderson, Greg and Pailoor, Shankara and Dillig, Isil and Chaudhuri, Swarat},
title = {Optimization and Abstraction: A Synergistic Approach for Analyzing Neural Network Robustness},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314614},
doi = {10.1145/3314221.3314614},
abstract = {In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and (δ -)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2, Reluplex, and Reluval.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {731–744},
numpages = {14},
keywords = {Robustness, Abstract Interpretation, Machine learning, Optimization},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{NEURIPS2018_2ecd2bd9,
 author = {Wang, Shiqi and Pei, Kexin and Whitehouse, Justin and Yang, Junfeng and Jana, Suman},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Efficient Formal Safety Analysis of Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/2ecd2bd94734e5dd392d8678bc64cdab-Paper.pdf},
 volume = {31},
 year = {2018}
}

@inproceedings{NEURIPS2021_fac7fead,
 author = {Wang, Shiqi and Zhang, Huan and Xu, Kaidi and Lin, Xue and Jana, Suman and Hsieh, Cho-Jui and Kolter, J. Zico},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {29909--29921},
 publisher = {Curran Associates, Inc.},
 title = {Beta-CROWN: Efficient Bound Propagation with Per-neuron Split Constraints for Neural Network Robustness Verification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/fac7fead96dafceaf80c1daffeae82a4-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{Boopathy_Weng_Chen_Liu_Daniel_2019,
    title={CNN-Cert: An Efficient Framework for Certifying Robustness of Convolutional Neural Networks},
    volume={33},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/4193},
    DOI={10.1609/aaai.v33i01.33013240},
    abstractNote={&lt;p&gt;Verifying robustness of neural network classifiers has attracted great interests and attention due to the success of deep neural networks and their unexpected vulnerability to adversarial perturbations. Although finding minimum adversarial distortion of neural networks (with ReLU activations) has been shown to be an NP-complete problem, obtaining a non-trivial lower bound of minimum distortion as a provable robustness guarantee is possible. However, most previous works only focused on simple fully-connected layers (multilayer perceptrons) and were limited to ReLU activations. This motivates us to propose a general and efficient framework, CNN-Cert, that is capable of certifying robustness on general convolutional neural networks. Our framework is general – we can handle various architectures including convolutional layers, max-pooling layers, batch normalization layer, residual blocks, as well as general activation functions; our approach is efficient – by exploiting the special structure of convolutional layers, we achieve up to 17 and 11 times of speed-up compared to the state-of-the-art certification algorithms (e.g. Fast-Lin, CROWN) and 366 times of speed-up compared to the dual-LP approach while our algorithm obtains similar or even better verification bounds. In addition, CNN-Cert generalizes state-of-the-art algorithms e.g. Fast-Lin and CROWN. We demonstrate by extensive experiments that our method outperforms state-of-the-art lowerbound-based certification algorithms in terms of both bound quality and speed.&lt;/p&gt;},
    number={01},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Boopathy, Akhilan and Weng, Tsui-Wei and Chen, Pin-Yu and Liu, Sijia and Daniel, Luca},
    year={2019},
    month={Jul.},
    pages={3240-3247} }

@INPROCEEDINGS{8418593,
  author={Gehr, Timon and Mirman, Matthew and Drachsler-Cohen, Dana and Tsankov, Petar and Chaudhuri, Swarat and Vechev, Martin},
  booktitle={2018 IEEE Symposium on Security and Privacy (SP)},
  title={AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation},
  year={2018},
  volume={},
  number={},
  pages={3-18},
  doi={10.1109/SP.2018.00058}}

@inproceedings{NEURIPS2019_0a9fdbb1,
 author = {Singh, Gagandeep and Ganvir, Rupanshu and P\"{u}schel, Markus and Vechev, Martin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Beyond the Single Neuron Convex Barrier for Neural Network Certification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0a9fdbb17feb6ccb7ec405cfb85222c4-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{NEURIPS2019_246a3c55,
 author = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/246a3c5544feb054f3ea718f61adfa16-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{linearOverapprox,
author = {Singh, Gagandeep and Gehr, Timon and P\"{u}schel, Markus and Vechev, Martin},
title = {An Abstract Domain for Certifying Neural Networks},
year = {2019},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {POPL},
url = {https://doi.org/10.1145/3290354},
doi = {10.1145/3290354},
abstract = {We present a novel method for scalable and precise certification of deep neural networks. The key technical insight behind our approach is a new abstract domain which combines floating point polyhedra with intervals and is equipped with abstract transformers specifically tailored to the setting of neural networks. Concretely, we introduce new transformers for affine transforms, the rectified linear unit (ReLU), sigmoid, tanh, and maxpool functions. We implemented our method in a system called DeepPoly and evaluated it extensively on a range of datasets, neural architectures (including defended networks), and specifications. Our experimental results indicate that DeepPoly is more precise than prior work while scaling to large networks. We also show how to combine DeepPoly with a form of abstraction refinement based on trace partitioning. This enables us to prove, for the first time, the robustness of the network when the input image is subjected to complex perturbations such as rotations that employ linear interpolation.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {41},
numpages = {30},
keywords = {Adversarial attacks, Abstract Interpretation, Deep Learning}
}

@inproceedings{MLSYS2021_ca46c1b9,
 author = {M\"{u}ller , Christoph and Serre, Fran\c{c}ois and Singh, Gagandeep and P\"{u}schel, Markus and Vechev, Martin},
 booktitle = {Proceedings of Machine Learning and Systems},
 editor = {A. Smola and A. Dimakis and I. Stoica},
 pages = {733--746},
 title = {Scaling Polyhedral Neural Network Verification on GPUs},
 url = {https://proceedings.mlsys.org/paper_files/paper/2021/file/ca46c1b9512a7a8315fa3c5a946e8265-Paper.pdf},
 volume = {3},
 year = {2021}
}

@InProceedings{Reluplex,
author="Katz, Guy
and Barrett, Clark
and Dill, David L.
and Julian, Kyle
and Kochenderfer, Mykel J.",
editor="Majumdar, Rupak
and Kun{\v{c}}ak, Viktor",
title="Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="97--117",
abstract="Deep neural networks have emerged as a widely used and effective means for tackling complex, real-world problems. However, a major obstacle in applying them to safety-critical systems is the great difficulty in providing formal guarantees about their behavior. We present a novel, scalable, and efficient technique for verifying properties of deep neural networks (or providing counter-examples). The technique is based on the simplex method, extended to handle the non-convex Rectified Linear Unit (ReLU) activation function, which is a crucial ingredient in many modern neural networks. The verification procedure tackles neural networks as a whole, without making any simplifying assumptions. We evaluated our technique on a prototype deep neural network implementation of the next-generation airborne collision avoidance system for unmanned aircraft (ACAS Xu). Results show that our technique can successfully prove properties of networks that are an order of magnitude larger than the largest networks verified using existing methods.",
isbn="978-3-319-63387-9"
}

@InProceedings{Marabou,
author="Katz, Guy
and Huang, Derek A.
and Ibeling, Duligur
and Julian, Kyle
and Lazarus, Christopher
and Lim, Rachel
and Shah, Parth
and Thakoor, Shantanu
and Wu, Haoze
and Zelji{\'{c}}, Aleksandar
and Dill, David L.
and Kochenderfer, Mykel J.
and Barrett, Clark",
editor="Dillig, Isil
and Tasiran, Serdar",
title="The Marabou Framework for Verification and Analysis of Deep Neural Networks",
booktitle="Computer Aided Verification",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="443--452",
abstract="Deep neural networks are revolutionizing the way complex systems are designed. Consequently, there is a pressing need for tools and techniques for network analysis and certification. To help in addressing that need, we present Marabou, a framework for verifying deep neural networks. Marabou is an SMT-based tool that can answer queries about a network's properties by transforming these queries into constraint satisfaction problems. It can accommodate networks with different activation functions and topologies, and it performs high-level reasoning on the network that can curtail the search space and improve performance. It also supports parallel execution to further enhance scalability. Marabou accepts multiple input formats, including protocol buffer files generated by the popular TensorFlow framework for neural networks. We describe the system architecture and main components, evaluate the technique and discuss ongoing work.",
isbn="978-3-030-25540-4"
}

@InProceedings{simplex-based,
author="Elboher, Yizhak Yisrael
and Gottschlich, Justin
and Katz, Guy",
editor="Lahiri, Shuvendu K.
and Wang, Chao",
title="An Abstraction-Based Framework for Neural Network Verification",
booktitle="Computer Aided Verification",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="43--65",
abstract="Deep neural networks are increasingly being used as controllers for safety-critical systems. Because neural networks are opaque, certifying their correctness is a significant challenge. To address this issue, several neural network verification approaches have recently been proposed. However, these approaches afford limited scalability, and applying them to large networks can be challenging. In this paper, we propose a framework that can enhance neural network verification techniques by using over-approximation to reduce the size of the network---thus making it more amenable to verification. We perform the approximation such that if the property holds for the smaller (abstract) network, it holds for the original as well. The over-approximation may be too coarse, in which case the underlying verification tool might return a spurious counterexample. Under such conditions, we perform counterexample-guided refinement to adjust the approximation, and then repeat the process. Our approach is orthogonal to, and can be integrated with, many existing verification techniques. For evaluation purposes, we integrate it with the recently proposed Marabou framework, and observe a significant improvement in Marabou's performance. Our experiments demonstrate the great potential of our approach for verifying larger neural networks.",
isbn="978-3-030-53288-8"
}

@inproceedings{
singh2018robustness,
title={Robustness Certification with Refinement},
author={Gagandeep Singh and Timon Gehr and Markus Püschel and Martin Vechev},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=HJgeEh09KQ},
}

@misc{lazarus2022mixed,
      title={A Mixed Integer Programming Approach for Verifying Properties of Binarized Neural Networks},
      author={Christopher Lazarus and Mykel J. Kochenderfer},
      year={2022},
      eprint={2203.07078},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{raghunathan2020certified,
      title={Certified Defenses against Adversarial Examples},
      author={Aditi Raghunathan and Jacob Steinhardt and Percy Liang},
      year={2020},
      eprint={1801.09344},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{dvijotham2018dual,
  title={A Dual Approach to Scalable Verification of Deep Networks.},
  author={Dvijotham, Krishnamurthy and Stanforth, Robert and Gowal, Sven and Mann, Timothy A and Kohli, Pushmeet},
  booktitle={UAI},
  volume={1},
  number={2},
  pages={3},
  year={2018}
}

@article{MULTIlABEL1,
title={Multi-label Adversarial Perturbations},
author={Qingquan Song; Haifeng Jin; Xiao Huang; Xia Hu},
year={2018},
journal={IEEE}
}

@INPROCEEDINGS{MULTIlABEL2,
  author={Zhou, Nan and Luo, Wenjian and Lin, Xin and Xu, Peilan and Zhang, Zhenya},
  booktitle={2020 International Joint Conference on Neural Networks (IJCNN)},
  title={Generating Multi-label Adversarial Examples by Linear Programming},
  year={2020},
  volume={},
  number={},
  pages={1-8},
  doi={10.1109/IJCNN48605.2020.9206614}}

@article{MULTIlABEL3,
    title={Characterizing the Evasion Attackability of Multi-label Classifiers},
    volume={35},
    url={https://ojs.aaai.org/index.php/AAAI/article/view/17273},
    DOI={10.1609/aaai.v35i12.17273},
    abstractNote={Evasion attack in multi-label learning systems is an interesting, widely witnessed, yet rarely explored research topic. Characterizing the crucial factors determining the attackability of the multi-label adversarial threat is the key to interpret the origin of the adversarial vulnerability and to understand how to mitigate it. Our study is inspired by the theory of adversarial risk bound. We associate the attackability of a targeted multi-label classifier with the regularity of the classifier and the training data distribution. Beyond the theoretical attackability analysis, we further propose an efficient empirical attackability estimator via greedy label space exploration. It provides provably computational efficiency and approximation accuracy. Substantial experimental results on real-world datasets validate the unveiled attackability factors and the effectiveness of the proposed empirical attackability indicator.},
    number={12},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Yang, Zhuo and Han, Yufei and Zhang, Xiangliang},
    year={2021},
    month={May},
    pages={10647-10655} }

@InProceedings{Hu_2021_ICCV,
    author    = {Hu, Shu and Ke, Lipeng and Wang, Xin and Lyu, Siwei},
    title     = {TkML-AP: Adversarial Attacks to Top-k Multi-Label Learning},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2021},
    pages     = {7649-7657}
}

@unpublished{melacci:hal-02971233,
  TITLE = {{Can Domain Knowledge Alleviate Adversarial Attacks in Multi-Label Classifiers?}},
  AUTHOR = {Melacci, Stefano and Ciravegna, Gabriele and Sotgiu, Angelo and Demontis, Ambra and Biggio, Battista and Gori, Marco and Roli, Fabio},
  URL = {https://hal.science/hal-02971233},
  NOTE = {working paper or preprint},
  YEAR = {2020},
  MONTH = Oct,
  HAL_ID = {hal-02971233},
  HAL_VERSION = {v1},
}

@ARTICLE{9857594,
  author={Kong, Linghao and Luo, Wenjian and Zhang, Hongwei and Liu, Yang and Shi, Yuhui},
  journal={IEEE Transactions on Artificial Intelligence},
  title={Evolutionary Multi-Label Adversarial Examples: An Effective Black-Box Attack},
  year={2022},
  volume={},
  number={},
  pages={1-12},
  doi={10.1109/TAI.2022.3198629}}

@misc{mahmood2022effective,
      title={Towards Effective Multi-Label Recognition Attacks via Knowledge Graph Consistency},
      author={Hassan Mahmood and Ehsan Elhamifar},
      year={2022},
      eprint={2207.05137},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}


