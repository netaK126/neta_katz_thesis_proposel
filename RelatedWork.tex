%! Author = julianmour
%! Date = 01/05/2023

\section{Related Work}
Our thesis is related to neural network verification, adversarial attacks against multi-label classifiers, and the relationships between one classifier to another.

\subsection{Robustness Verifiers for Neural Networks}\label{subsec:verifiers}
Verifiers of neural networks evaluate the safety characteristics of neural networks.These verifiers use a variety of mathematical methods, including Mixed-Integer Linear Programming (MILP) ~\cite{singh2018robustness,lazarus2022mixed}, over approximation ~\cite{qin2019verification}, and abstract interpolation ~\cite{ABSTRACTINTER,INCOMPLETE1}. Verification techniques fall under different categories based on the guarantees they provide:
\begin{itemize}
    \item  Complete verifiers: provide a definitive answer regarding the validity of a specified attribute. Usually, these approaches transform verification problems into constraints that can be solved using solvers like SAT ~\cite{SATAPPROACH1,SATAPPROACH2} or SMT ~\cite{NNTOBINARCONSTRAINS,PLANET,Reluplex}. Complete verifiers usually have long execution time and thus do not scale to large networks.
        In our preliminary research, we rely on a complete verifier~\cite{MIPVERIFY}.
    \item Incomplete verifiers: those verifiers often rely on approximate techniques which scale better ~\cite{INCOMPLETE1,INCOMPLETE2}, although they do not necessarily provide a definite answer and may return \emph{unknown} for a given property. 
\end{itemize}

\subsection{Robustness Property}
Intuitively, a classier is globally robust to a given perturbation if for any input for which the network is confident enough in its classification, this perturbation does not cause the network to change its classification ~\cite{GLOBALROBUSNN}. For such inputs, it is inevitable that inputs near the decision boundaries, where the network’s classification confidence is low, are misclassified when adding perturbations. However, we would like to verify the network only on the space of meaningful inputs, while this property considers any input so it may lead to as other properties discussed in ~\cite{Reluplex,EFCIENTGLOBALROBU}. Therefore, we focus on the minimal confidence of a network for any input, such that the network do not misclassified under this perturbation ~\cite{VHAGAR}.
     
\subsection{Differential Specifications of Similar Classifiers}
There are many neural networks that perform the same task. More specifically, many classifiers are trained for the same purpose. Therefore, validating the robustness of several classifiers of the same task is beneficial. Moreover, utilizing the relationship between those classifiers can scale the evaluation process and reduce the running time. DeepXplore ~\cite{DEEPXPLORE} leveraged several Deep Learning (DL) systems that perform the same task to cross-validate each other’s predictions. By comparing the outputs of these systems for the same input, differences can indicate potential errors. When comparing those systems, they refer to the output layer alone. ReluDiff ~\cite{RELUDIFF} focuses on differential verification of two feed-forward networks of the same task and structure while differing only in the numerical values of edge weights. Although ReluDiff improves naive approach it can suffer from severe error-explosion.   NeuroDiff ~\cite{NEURODIFF} is a symbolic and fine-grained approximation technique that reduces the approximation error introduced when a neuron is in an unstable state, and mitigates the explosion of such approximation error after
it is introduced. DiffRNN ~\cite{DIFFRNN} is another algorithm addressing the differential behavior of two neural networks. While ReluDIFF focuses specifically on the ReLU activation function, DiffRNN is more general and applicable to a wider range of RNN architectures. ~\cite{CFXROBUSTNESS} used relational verification as a formal framework to reason about the local robustness of counterfactual explanations (CFXs), which are explanations provided to help understand why a machine learning model made a certain prediction, under model multiplicity, where multiple models achieve the same accuracy on a prediction task, but differ in their internals ~\cite{PREDICTIVEMULTIPICITY}, using MILP. Another approach called QEBVerif ~\cite{QEBVERIF} encodes both the model and its quantized version into a MILP verifier, and applies symbolic interval analysis on the two networks based on which some activation patterns could be omitted. Their motivation is to reduce the size of mixed integer linear constraints and boost the verification while maintaining a sound and complete verification algorithm. 