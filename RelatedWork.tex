%! Author = julianmour
%! Date = 01/05/2023

\section{Related Work}
Our thesis is related to neural network verification, adversarial attacks against multi-label classifiers, and the relationships between one classifier to another.

\subsection{Neural Network Verification}\label{subsec:verifiers}
Verifiers of neural networks evaluate the safety characteristics of neural networks.These verifiers use a variety of mathematical methods, including model checking and constraint solving, to measure neural network performance and assess whether the specified condition is true. Verifiers can be divided into two main categories:
\begin{itemize}
    \item  Complete verifiers: provide a definitive answer regarding the validity of a specified attribute.
    Complete verifiers usually have long execution time and thus do not scale to large networks.
        In our preliminary research, we rely on a complete verifier~\cite{MIPVERIFY}.
    \item Incomplete verifiers: those verifiers often rely on approximate techniques which scale better~\cite{INCOMPLETE1, INCOMPLETE2}, although they do not necessarily provide a definite answer regarding, and may also return \emph{unknown} for a given property. 
\end{itemize}


\subsection{Adversarial Attacks in Multi-Label Classification}
In an adversarial attack, an adversary intentionally perturbs an input to the classifier to cause misclassification.
In multi-label classification, each input can be assigned multiple classes.
Adversarial attacks in this setting can be defined with respect to various goals, such as causing the classifier to predict a different incorrect subset of classes or not predicting a given correct class.
We focus on the latter goal.

\subsection{Robustness Property}
Intuitively, a classier is globally robust to a given perturbation if for any input for which the network is confident enough in its classification, this perturbation does not cause the network to change its classification. For such inputs, it is inevitable that inputs near the decision boundaries, where the network’s classification confidence is low, are misclassified when adding perturbations. However, we would like to verify the network only on the space of meaningful inputs, while this property considers any input so it may lead to ~\cite{Reluplex}. Therefore, we focus on the minimal confidence of a network for any input, such that the network do not misclassified under this perturbation ~\cite{VHAGAR}.
     
\subsection{Robustness Verifiers of Multiple Classifiers}
There are many neural networks that perform the same task. More specifically, many classifiers are trained for the same purpose. Therefore, validating the robustness of several classifiers of the same task is beneficial. Moreover, utilizing the relationship between those classifiers can scale the evaluation process and reduce the running time. DeepXplore ~\cite{DEEPXPLORE} leveraged several Deep Learning (DL) systems that perform the same task to cross-validate each other’s predictions. By comparing the outputs of these systems for the same input, differences can indicate potential errors. When comparing those systems, they refer to the output layer alone. ReluDiff ~\cite{RELUDIFF} focus on differential verification of two feed-forward networks of the same task and structure while differing only in the numerical values of edge weights. Although ReluDiff improves naive approach it can suffer from severe error-explosion.   NeuroDiff ~\cite{NEURODIFF} is a symbolic and fine-grained approximation technique that reduces the approximation error introduced when a neuron is in an unstable state, and mitigates the explosion of such approximation error after
it is introduced. DiffRNN ~\cite{DIFFRNN} is another algorithm addressing differential behavior of two neural networks. While ReluDIFF focuses specifically on ReLU activation function, DiffRNN is more general and applicable to a wider range of RNN architectures. 