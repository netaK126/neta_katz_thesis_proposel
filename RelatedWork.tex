%! Author = julianmour
%! Date = 01/05/2023

\section{Related Work}
Our thesis is related to neural network verification, adversarial attacks against multi-label classifiers, and the relationships between one classifier to another.

\subsection{Robustness Verifiers for Neural Networks}\label{subsec:verifiers}
Verifiers of neural networks evaluate the safety characteristics of neural networks.These verifiers use a variety of mathematical methods, including Mixed-Integer Linear Programming (MIP) ~\cite{singh2018robustness,lazarus2022mixed}, over approximation ~\cite{qin2019verification}, and abstract interpolation ~\cite{ABSTRACTINTER,INCOMPLETE1}. Verification techniques fall under different categories based on the guarantees they provide:
\begin{itemize}
    \item  Complete verifiers: provide a definitive answer regarding the validity of a specified attribute. Usually, these approaches transform verification problems into constraints that can be solved using solvers like SAT ~\cite{SATAPPROACH1,SATAPPROACH2} or SMT ~\cite{NNTOBINARCONSTRAINS,PLANET,Reluplex}. Complete verifiers usually have long execution time and thus do not scale to large networks.
        In our preliminary research, we rely on MIP~\cite{MIPVERIFY}.
    \item Incomplete verifiers: those verifiers often rely on approximate techniques which scale better ~\cite{INCOMPLETE1,INCOMPLETE2}, although they do not necessarily provide a definite answer and may return \emph{unknown} for a given property. 
\end{itemize}

\subsection{Robustness Property}
Many studies provide different properties to examine the global robustness of a neural network.
Some evaluate the proprety of worst-case change in a network’s outputs. However, this proprety cannot determine whether the classification changes ~\cite{Reluplex,EFCIENTGLOBALROBU}. Others use local robustness guarantees to estimate global robustness for classifiers Another~\cite{MEASURENNROBCON,GLOBALPROPERTY}. Another approach is to evaluate the robustness of different input regions extracted from the database, without implying on global robustness  ~\cite{ROBUSTFROMDATASET}. Moreover, researchers show that most known layers of DNNs are Lipschitz continuous, and presents a verification approach based on global optimization ~\cite{ANOTHERGLOBALPROPERTY}. Lipschitz bounds can be used to  train \( L_\infty \) globally robust networks ~\cite{RETHINKLIP}. In addition, we can provide probabilistic guarantees of global robustness ~\cite{GROMA}.

Intuitively, a classifier is globally robust to a given perturbation if for any input for which the network is confident enough in its classification, this perturbation does not cause the network to change its classification ~\cite{GLOBALROBUSNN}. For such inputs, it is inevitable that inputs near the decision boundaries, where the network’s classification confidence is low, are misclassified when adding perturbations. However, we aim to verify the network only on the space of meaningful inputs, while this property considers any input so it may lead to as other properties ~\cite{Reluplex,EFCIENTGLOBALROBU}. Therefore, we focus on the minimal confidence of a network for any input, such that the network do not misclassified under this perturbation ~\cite{VHAGAR}.
     
\subsection{Differential Specifications of Similar Classifiers}
There are many neural networks that perform the same task, specifically classifiers. Validating the robustness of several classifiers of the same task is beneficial in several aspects, such as improving accuracy, enhancing reliability, detecting vulnerabilities, and ensuring consistency across different models.

DeepXplore ~\cite{DEEPXPLORE} finds incorrect corner case behaviors that cause the classifiers to misclassify in the domain of self-driving cars. DeepXplore compares the outputs of different deep learning systems with the same task to cross-validate each other’s predictions, when big differences indicate potential errors. DeepXplore considers systems with different architectures, therefore its ability to leverage dependencies between the architectures' layers and neurons is limited. ReluDiff ~\cite{RELUDIFF} address this limitation by focusing on differential verification of two closely related neural networks. Specifically, feed-forward networks of the same task and structure differ only in the numerical values of edge weights. Although ReluDiff improves naive approach it can suffer from severe error-explosion. NeuroDiff ~\cite{NEURODIFF} answers this issue. It is a symbolic and fine-grained approximation technique that reduces the approximation error introduced when a neuron is in an unstable state and mitigates the explosion of such approximation error. While ReluDiff and NeuroDiff discuss feed-forward networks, DiffRNN ~\cite{DIFFRNN} is a differential method for recurrent neural networks (RNNs). DiffRNN certifies the equivalence of a compressed network and the original network. Another approach called QEBVerif ~\cite{QEBVERIF} also examines the relationship between a quantized network and the original. It encodes both the model and its quantized version into a MIP verifier and applies symbolic interval analysis on the two networks based on which some activation patterns can be omitted. Their motivation is to reduce the size of mixed integer linear constraints and boost the verification while maintaining a sound and complete verification algorithm. Moreover, ~\cite{CFXROBUSTNESS} introduces a verification method for local robustness of counterfactual explanations (CFXs). CFXs are explanations provided to help understand why a machine learning model made a certain prediction. Some researchers use relational verification as a formal framework for reasoning the local robustness of CFXs under model multiplicity  ~\cite{CFXROBUSTNESS}, where multiple models achieve the same accuracy on a prediction task, but differ in their internals ~\cite{PREDICTIVEMULTIPICITY}, using MIP.

