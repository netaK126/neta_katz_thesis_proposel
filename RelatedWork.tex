%! Author = julianmour
%! Date = 01/05/2023

\section{Related Work}
Our thesis is related to neural network verification and in particular verification of global robustness properties and analysis of relationships between two networks.

\paragraph{Verifiers for neural networks}\label{subsec:verifiers}
Verifiers of neural networks evaluate the safety characteristics of neural networks. These verifiers use a variety of mathematical methods, including Mixed-Integer Linear Programming (MILP) ~\cite{singh2018robustness,lazarus2022mixed}, over approximation ~\cite{qin2019verification}, and abstract interpolation ~\cite{ABSTRACTINTER,INCOMPLETE1}. Verification techniques fall under different categories based on the guarantees they provide:
\begin{itemize}[nosep,nolistsep]
    \item  Complete verifiers: provide a definitive answer regarding the validity of a specified attribute. Usually, these approaches transform verification problems into constraints that can be solved using solvers like SAT ~\cite{SATAPPROACH1,SATAPPROACH2} or SMT ~\cite{NNTOBINARCONSTRAINS,PLANET,Reluplex}. %Complete verifiers usually have long execution time and thus do not scale to large networks.
        %In our preliminary research, we rely on MIP~\cite{MIPVERIFY}.
    \item Incomplete verifiers: those verifiers often rely on approximate techniques which scale better ~\cite{INCOMPLETE1,INCOMPLETE2}, although they generally cannot provide a definite answer. 
\end{itemize}

\paragraph{Global robustness verification}
A large body of work analyzes different global robustness properties of a neural network.
Some compute the worst-case change in a network's outputs in a given neighborhood~\cite{Reluplex,EFCIENTGLOBALROBU}. However, this property cannot capture whether the classification changes. Others use local robustness guarantees to estimate global robustness for classifiers~\cite{MEASURENNROBCON,GLOBALPROPERTY}. Another work evaluates the robustness in different input regions extracted from the database, which aims at approximating a global robustness property~\cite{ROBUSTFROMDATASET}. A different approach leverages the fact that most known layers of DNNs are Lipschitz continuous and present a verifier relying on global optimization~\cite{ANOTHERGLOBALPROPERTY}. The Lipschitz bound has also been leveraged to train \( L_\infty \) globally robust networks~\cite{RETHINKLIP}. A different approach  provides probabilistic guarantees of global robustness~\cite{GROMA}.

%\Dana{this part does not belong here:}
%NETA - I think this part is essential for understanding our research. where should we add it ?
%Intuitively, a classifier is globally robust to a given perturbation if for any input for which the network is confident enough in its classification, this perturbation does not cause the network to change its classification ~\cite{GLOBALROBUSNN}. For such inputs, it is inevitable that inputs near the decision boundaries, where the networkâ€™s classification confidence is low, are misclassified when adding perturbations. However, we aim to verify the network only on the space of meaningful inputs, while this property considers any input so it may lead to as other properties ~\cite{Reluplex,EFCIENTGLOBALROBU}. Therefore, we focus on the minimal confidence of a network for any input, such that the network do not misclassified under this perturbation ~\cite{VHAGAR}.
     
\paragraph{Differential specifications of similar classifiers}
%There are many neural networks that perform the same task, specifically classifiers. Validating the robustness of several classifiers of the same task is beneficial in several aspects, such as improving accuracy, enhancing reliability, detecting vulnerabilities, and ensuring consistency across different models.
Several works design algorithms to understand the similarities and differences of similar networks.
DeepXplore~\cite{DEEPXPLORE} computes inputs that cause classifiers to misclassify. 
To this end, it compares the outputs of multiple networks trained for the same task and looks for inputs that are classified differently by the networks. It relies on numerical optimization, and thus has no formal guarantees. %DeepXplore considers systems with different architectures, therefore its ability to leverage dependencies between the architectures' layers and neurons is limited. 
ReluDiff~\cite{RELUDIFF} computes the maximal difference of the output of two closely related neural networks, differing only in the values of their weights for any input within an input region of interest. However, it suffers from severe error-explosion. NeuroDiff~\cite{NEURODIFF} relies on a symbolic and fine-grained approximation technique to reduce this approximation error.
DiffRNN~\cite{DIFFRNN} targets on recurrent neural networks (RNNs) and verifies that a compressed network and the original network are equivalent. Two networks are considered equivalent if, for any input within a specified input region, the difference between their outputs is sufficiently close, as determined by an input bounding box using an SMT solver.
%\Dana{how? is it also a numerical bound on the output?}.
QEBVerif~\cite{QEBVERIF} examines the relationship between a quantized network and an original network, in terms of local robustness. QEBVerif determines whether the quantization error of a DNN remains within a specified bound. 
%\Dana{what is the output? is it a numerical bound?}
It first uses initial differential reachability analysis to check if the quantization error of a neural network is within bounds to reduce the problem's complexity, and if this fails, it converts the problem into a MILP problem to verify the error using standard solvers.  % Their motivation is to reduce the size of mixed integer linear constraints and boost the verification while maintaining a sound and complete verification algorithm. 
Another work introduces a verification method for local robustness of counterfactual explanations (CFXs) ~\cite{CFXROBUSTNESS}.
%\Dana{is this also over two similar networks?}
 CFXs are explanations provided to help understand why a machine learning model made a certain prediction. Some researchers use relational verification as a formal framework for reasoning the local robustness of CFXs under model multiplicity  ~\cite{CFXROBUSTNESS}, where multiple models achieve the same accuracy on a prediction task, but differ in their internals ~\cite{PREDICTIVEMULTIPICITY}, using MIP. \cite{CFXROBUSTNESS} focuses on feed-forward neural networks, using ReLU activation trained to solve binary classification tasks.
 %\Dana{not sure I understand, is this the setting of comparing two similar networks?}.
 %NETA - as I understand this paper, they start with similar networks but relax their solution to arbitrary models that represent linear functions using MILP.
