%! Author = julianmour
%! Date = 01/05/2023

\section{Problem Definition}

\sloppy
\paragraph{Multi-label classifier}
An image multi-label classifier $D$ is a function mapping a two-dimensional image $x\in \mathbb{R}^{n \times m}$ to a score vector over the possible set of classes $C=\{1,\ldots,c\}$, that is:
$D: \mathbb{R}^{n \times m} \rightarrow {\mathbb{R}}^{|C|}$.
This research would focus on two multi-label classifiers of the same task, with similar architectures.

\paragraph{Perturbation}
Give an input $x$ and a perturbation vector $\epsilon'=(\epsilon_1,...,\epsilon_k)\in{\mathbb{R}}^{|k|}$
, a perturbation is a function $f_P(x,\epsilon')$ defining how the input $x$ is perturbed. Each perturbation vector $\epsilon'$ is in a given range $I_\epsilon$, denoted as $\epsilon'\in I_\epsilon$ if its entries are in their intervals: $\forall{k}$ $(\epsilon')_k\in (I_\epsilon)_k$.

\paragraph{Global Robustness of a Single Network}
Given a perturbation $P$ and its range $I_\epsilon$, a naive approach for global robustness is:
$\forall{x}\forall{\epsilon'}\in{I_\epsilon}$  argmax($D(x)$) = argmax($f_P(x,\epsilon')$).
However, this property is problematic because some inputs $x$ are too close to the classifier's decision boundaries and violate this property ~\cite{DECISIONBOUND}. As a result it is impossible to satisfy it for non-trivial classifier. Thus, this study focuses on inputs whose networks' confidence in the classification is high enough ~\cite{VHAGAR}:\\
Given a classifier $D$, an input $x$ and a class $c'\in{C}$, the class confidence of $D$ in $c'$ is:
$$\mathcal{C}(x,c',D)=D(x)_{c'}-max_{c''\ne c'}(D(x)_{c''})$$
For a single network, we define the property $\delta-Global Robustness$ as follows:\\
Given a classifier $D$, an input $x$ and a class $c'\in{C}$, a perturbation $P$, a range $I_\epsilon$ and a class confidence $\delta>0$, the classifier $D$ is $\delta$-globally robust for $c'$ under $(P,I_\epsilon)$ if:\\
$$\forall{x}\forall{\epsilon'}\in{I_\epsilon}:    \mathcal{C}(x,c',D) \geq \delta \rightarrow argmax(D(x)) = argmax(f_P(x,\epsilon')) $$
Intuitively, the classifier is robust for any confidence higher or equal to the minimal $delta$. However, this definition cannot be solved via MIP solvers. Therefore, we need to adjust the definition to MIP solvers and consider the dual problem: computing the maximal globally robust bound. This way we would look for the maximal $\delta$ that violates the constraint. Overall, for a single classifier the property we would like to evaluate is:

$$\max{\delta}, subject-to. \exists{x}\exists{\epsilon'}\in{I_\epsilon}: \mathcal{C}(x,c',D) \geq \delta \rightarrow \mathcal{C}(f_P(x,\epsilon'),c',D)\leq 0$$

\paragraph{Problem definition} Given two classifiers $F_1,F_2: \mathbb{R}^{n \times m} \rightarrow {\mathbb{R}}^{|C|}$ with the same architecture, a class $c\in{C}$, a perturbation $P$ and its range $I_\epsilon$ the goal is to compute maximal $\delta$, where there $\exists{x}\exists{\epsilon'}\in{I_\epsilon}$ satisfying:

\begin{enumerate}[nosep,nolistsep]
    \item $\mathcal{C}_{F_1}(x,c',D) \geq \delta$
    \item $\mathcal{C}_{F_2}(x,c',D) > 0$
    \item $\mathcal{C}_{F_1}(f_P(x,\epsilon'),c',D)\leq 0$
    \item $\mathcal{C}_{F_2}(f_P(x,\epsilon'),c',D)\leq 0$
\end{enumerate}