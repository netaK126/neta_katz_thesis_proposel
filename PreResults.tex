%! Author = julianmour
%! Date = 01/05/2023

\section{Preliminary Results}
In this section, we describe our preliminary results.
We evaluate our approach on classifiers for the MNIST dataset, consisting of images showing a single digit.
The classifier's goal is to return the digit shown in the image. In our experiments, we focus on the global robustness property for the class $c=0$ and \Dana{add two more classes}.

We consider two network architectures: 
a fully connected network consisting of three layers, ten neurons in each, denoted \texttt{3x10}, and a convolutional neural network. 
We consider two variations of the convolutional networks, where one is trained with one more iteration. \ref{table_architectures} describes our models. For each model $D$, we define $D_1$ as $D$ and $D_2$ as $D$ with a slight change in a single weight in the last layer on $D$\Dana{which weight? what is the difference in the value?}. We provide the MILP solver a timeout of 50800 seconds (14 hours, 6 min, and 40 seconds)\Dana{too specific timeout, how did you choose it? also super long, two hours is not enough?}. We run the naive approach (without any constraints), called \emph{Baseline}, and our approach (with dependency constraints):
\Dana{what is the perturbation?}
Like~\cite{VHAGAR}, our algorithm is anytime, namely it computes lower and upper bounds on the desired class confidence $\delta_c$.

\begin{table}[H]
    \centering
    \resizebox{\textwidth}{!}{
    \begin{tabular}{@{\extracolsep{\fill}}llll@{}}
        \toprule
        \makecell{Dataset} & \makecell{Name} & \makecell{Architecture}  & \makecell{Iterations during training} \\
        \midrule            
        \multirow{3}{*}{MNIST} & 3 x 10 & 3 fully-connected layers & 20 \\
                               & CNN1 & 2 convolutional (stride 4) and 2 fully-connected layers & 20 \\
                               & CNN2 & 2 convolutional (stride 4) and 2 fully-connected layers & 19 \\
        \bottomrule
    \end{tabular}
    }
    \caption{The networks used for this experience.
        \label{table_architectures}}
\end{table}

Figure~\ref{fig:3_x_10}, Figure~\ref{fig:cnn0_1} and Figure~\ref{fig:cnn0_2} show the gaps in the execution time for the 3X10, CNN1, and CNN2 respectively.
Results show that our approach significant reduces the execution time, on average by 82\%. 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{3x10.png}
  \caption{The execution time of the baseline against our approach, when the model type is a fully connected network 3x10 .}
  \label{fig:3_x_10}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{cnn0_1.png}
  \caption{The execution time of the baseline against our approach, when the model type is a convolutional neural network, trained using 20 iterations\Dana{shortened it, call it for example conv-20}.}
  \label{fig:cnn0_1}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{cnn0_2.png}
  \caption{The execution time of the baseline against our approach, when the model type is a convolutional neural network, trained using 19 iterations .}
  \label{fig:cnn0_2}
\end{figure}

We next report the lower and upper bounds on $\delta_c$, computed within the timeout.
Table~\ref{table_3_x_10} shows the bounds for the \texttt{3x10} network. It shows that both approach obtain comparable bounds, with the baseline obtaining slightly tighter bounds. Table~\ref{table_cnn0_1} and Table~\ref{table_cnn0_2} show the results for the convolutional networks. Results show that our approach enables to improve tighter bounds by\Dana{complete}\%. %This is due to timeout. the base approach is much slower, and does not manage to find tighter bound for more complex architectures before reaching timeout. 
Namely, preliminary results show that our approach enhances scalability and consequently its accuracy. %is similar to the base approach for the tested scenario of two almost identical networks, besides a singular weight. Our goal is to further extend this improvement to other scenarios as well. 

\begin{table}[H]
	\centering
    \resizebox{\textwidth}{!}{
	\begin{tabular}{@{\extracolsep{\fill}}llllll@{}}
		\toprule
		& \makecell{ } & \makecell{Brightness[0.1] \\ Weight addition[0.1]} & \makecell{Brightness[0.1] \\ Weight addition[0.05]} & \makecell{Brightness [0.05] \\ Weight addition[0.1]} & \makecell{Brightness [0.05] \\ Weight addition[0.05]}\\
		\midrule			
		\multirow{2}{*}{Base approach} & upper bound  & 3.27 & 3.3 &  1.79 & 1.79\\
                              & lower bound & 3.27 & 3.27 &  1.79 & 1.79\\
		\midrule
		\multirow{2}{*}{Our approach} & upper bound  & 3.3 & 3.3 & 1.8 & 1.79\\
                              & lower bound & 3.27 & 3.27 & 1.79 & 1.79\\						
		\bottomrule
	\end{tabular}
}
\caption{The lower and upper bounds for 3X10 architecture 
		\label{table_3_x_10}}
\end{table}


\begin{table}[H]
	\centering
    \resizebox{\textwidth}{!}{
	\begin{tabular}{@{\extracolsep{\fill}}llllll@{}}
		\toprule
		& \makecell{ } & \makecell{Brightness[0.1] \\ Weight addition[0.1]} & \makecell{Brightness[0.1] \\ Weight addition[0.05]} & \makecell{Brightness [0.05] \\ Weight addition[0.1]} & \makecell{Brightness [0.05] \\ Weight addition[0.05]}\\
		\midrule			
		\multirow{2}{*}{Base approach} & upper bound  & 19.39 & 17.79 & 15.45 & 19.27\\
                              & lower bound & 15.01 & 15.01 &  12.18 & 12.19\\
		\midrule
		\multirow{2}{*}{Our approach} & upper bound  & 15.16 & 15.15 & 12.59 & 12.59\\
                              & lower bound & 15.01 & 15.01 &  12.46 & 12.46\\					
		\bottomrule
	\end{tabular}
}
\caption{The lower and upper bounds for CNN architecture with 20 iteration during training process.
		\label{table_cnn0_1}}
\end{table}


\begin{table}[H]
	\centering
    \resizebox{\textwidth}{!}{
	\begin{tabular}{@{\extracolsep{\fill}}llllll@{}}
		\toprule
		& \makecell{ } & \makecell{Brightness[0.1] \\ Weight addition[0.1]} & \makecell{Brightness[0.1] \\ Weight addition[0.05]} & \makecell{Brightness [0.05] \\ Weight addition[0.1]} & \makecell{Brightness [0.05] \\ Weight addition[0.05]}\\
		\midrule			
		\multirow{2}{*}{Base approach} & upper bound  & 15.77 & 17.71 & 15.6 & 19.16\\
                              & lower bound & 14.18 & 14.18 & 11.81 & 11.73\\
		\midrule
		\multirow{2}{*}{Our approach} & upper bound & 14.29 & 14.31 & 11.92 & 11.92\\
                              & lower bound & 14.15 & 14.17 & 11.8 & 11.81\\				
		\bottomrule
	\end{tabular}
}
\caption{The lower and upper bounds for CNN architecture with 19 iteration during training process.
		\label{table_cnn0_2}}
\end{table}
% NETA - image of database + explain: "hey we did good - but not good enough". 

% bounds in table
% times show in graph 