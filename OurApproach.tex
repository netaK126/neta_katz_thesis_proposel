%! Author = julianmour
%! Date = 01/05/2023

\section{Our Approach}

In this thesis, we will design an algorithm that given a multi-label classifier and an image computes the maximal neighborhood given by an epsilon sequence, describing how perturbation of the non-target object affects the classification of the target object.
A naive algorithm computes the epsilon series one by one, where at each iteration it computes the maximal epsilon using a binary search.
However, this approach is suitable in case the weight vector poses a strict ordering on the importance of the layers, and it also has a high time overhead.
%Problem is this approach is very expensive in time hence inefficient.
Instead, we aim to build on the oracle-guided numerical verification proposed in~\cite{MARVEL}, to obtain a scalable algorithm.
Technically, our algorithm has three main components, that iteratively interact with one another:
\begin{itemize}[nosep,nolistsep]
    \item A local robustness verifier: Given a classifier and a layered neighborhood ${N^{o}_\epsilon}(x)$ of an image $x$ and object $o$, it determines whether the classifier is robust in this neighborhood.
        \item A numerical optimizer: Given a classifier and a robust layered neighborhood, it attempts to expand the neighborhood into a larger robust neighborhood, with respect to the weight vector. %by expanding it more, as we intend to maximize the norm of the neighborhood.
    \item A counterexample-guided inductive synthesiser (CEGIS): Given a classifier and a \emph{non-robust} layer-neighborhood, it attempts to identify directions of the previously robust neighborhood that cannot be further expanded. 
    \end{itemize}
The components interact until the neighborhood cannot be further expanded.
We next provide details about the different components and explain the open challenges.

    \paragraph{The verifier}
    There are many verifiers that can reason about the local robustness of a (single-label) classifier.
    However, none addresses a multi-label classifier.
    Thus, part of the challenge is adapting a verifier to multi-label classification.
    In particular, the verifier only has to prove that one of the labels is the target object's label. %Since this is a multi-label classifier, we are interested only in the target class robustness;
    %The verifier will say that the neighborhood is robust if and only if all images in the neighborhood are classified to the target class.
    In our preliminary research, we rely on the mixed-integer linear program (MILP) based verifier, called MIPVerify~\cite{MIPVERIFY}.
    This verifier encodes the robustness task into a MILP maximization problem and uses the Gurobi Optimizer to solve it.
    We adapt the verifier to multi-label classifiers by adapting the objective function.
    For a single-label classifier, the objective function is the difference between the highest score and the score of the target class:
    \begin{gather*}
        \max_{c \in C, c \neq c_{t}}\{F(x')_c- F(x')_{c_{t}}\mid x'\in N(x)\}
    \end{gather*}
    If the difference is negative, the neighborhood is robust. Otherwise, it is not robust.
    We call inputs in $N(x)$ maximizing this objective function the \emph{weakest points}, because they are the closest to the decision boundary.
%     A negative value of the maximized objective indicates a robust neighborhood.

    We adapt this objective function to multi-label classifiers as follows.
    Since classification is a set of the classes, instead of comparing to the highest score, we compare to the second highest score:
    %Meaning that the objective now represents the difference between the second-highest score of an incorrect class and the score of the target class ($c_{t}$).
    \begin{gather*}
        2^{nd}\max_{c \in C, c \neq c_{t}}\{F(x')_c- F(x')_{c_{t}}\mid x'\in N(x)\}
    \end{gather*}
    A negative value indicates that the neighborhood is robust with respect to $c_t$.
    %Beyond determining whether a neighborhood is robust or not, the verifier returns a set of points maximizing the objective, to which we call \emph{the weakest points of the image}.
    As before, the inputs maximizing this objective are called the weakest points.
    These points later help the optimizer to identify robust directions to expand the current neighborhood.
    %These are the points in the checked neighborhood that are least robust and the verifier will pass them to the optimizer as well.
    
    \paragraph{The optimizer}
    Our optimizer follows the approach of~\cite{MARVEL} and expands a robust neighborhood by computing the gradient of the optimization problem defined in the previous section.
    Since it is a constrained optimization, we relax the constraints and add equivalent terms to the optimization goal, similarly to ~\cite{MARVEL}.
    %We call this term \emph{the robustness level} (RL).\Dana{to define it, you need to present here the optimization function corresponding to the one on page 7 before}
    %To expand a robust neighborhood, our optimizer 
    %computes the gradients of both terms.
    %The norm's gradient is computed in a straightforward way while the RL's gradient is computed using the weakest points found by the verifier.
    Given the gradients, the optimizer expands the neighborhood by a small step and submits to the verifier. 
%    \Dana{is there an adaptation to mulit-label or is it like Marvel? we need to describe it}
%    \Julian{No adaptation in this part}
    %At last, we calculate the step gradient, which is a combination of both previous gradients, and expand the neighborhood towards the step gradient.
    
    \paragraph{The CEGIS component}
    The CEGIS component takes a non-robust neighborhood and the previously robust neighborhood and attempts to identify directions that must be shrunk towards making the (non-robust) neighborhood robust. %If the neighborhood checked by the verifier isn't robust, then we skip the optimization part.
    %Instead, we try to minimally shrink the neighborhood so that its robust.
    Computing the exact directions requires an exponential number of queries to the verifier.
    Instead, we shrink the non-robust neighborhood in the direction of the previously robust neighborhood.
    In our preliminary research,
    we shrink each layer according to a given weight vector, called \emph{the cutting weight}, which may depend on the input: $cw_x = (cw_0^x, cw_1^x, \ldots, cw_r^x)$.
    %The layer's cutting weight determines how much should the layer be shrunk towards the previously robust neighborhood.
    %We notate the cutting weights vector with $cw_x$, and its complement vector with $cw_x^*$ and define it as follows:
%    \Dana{what is a cutting vector? you need to first provide a high level explanation before the math notation};
    %\begin{gather*}
    %    cw_x = (cw_0^x, cw_1^x, \ldots, cw_r^x)\\
    %    cw_x^* = (1 - cw_0^x, 1 - cw_1^x, \ldots, 1 - cw_r^x)\\
    %\end{gather*}
    Mathematically, this translates to computing a new epsilon sequence $\varepsilon_x'$ that is a weighted average of the current non-robust epsilon sequence $\varepsilon_x$ and the previously robust neighborhood $\varepsilon_x^*$.
    We use \emph{element-wise multiplication} ($\odot$) to multiply each element in the epsilon sequences with its corresponding weight in the weights vectors:
%    \Dana{we cannot place all math notation together; please fix it so it looks like the previous section}.
    $
        {\varepsilon_x}' = \varepsilon_x \odot cw_x^T + \varepsilon_x^* \odot {cw_x^{-1}}^T
    $, where $ cw_x^{-1} = (1 - cw_0^x, 1 - cw_1^x, \ldots, 1 - cw_r^x)$. 
%    \Dana{what is the $\odot$ operation?}
    We consider two definitions for the cutting weights:
    \begin{itemize}[nosep,nolistsep]
        \item Fixed weights shrinking the neighborhood more in layers that are far from the non-target object and less in layers that are close to it:
        \begin{gather*}
            cw_x = ({\frac{r}{r+1}}^5, {\frac{r-1}{r+1}}^5, {\frac{r-2}{r+1}}^5, \ldots, 0)
        \end{gather*}
%        \Dana{not clear, can you define it mathematically?}
        \item Sensitivity weights shrinking more in layers that are less sensitive to perturbations.
        The sensitivity weights of an image are computed by using the Vanilla Gradient method, introduced by Simonyan et al.~\cite{VANILLAGRADIENT}.
        This method computes the sensitive pixels in an image by computing the gradient of a loss function with respect to the image pixels using backpropagation.
        Pixels with large gradients are likely to be more sensitive to perturbations, and perturbing them is likely to affect the classifier's robustness more.
        We translate the problem of finding sensitive pixels to finding sensitive layers by averaging all pixels' gradients in each layer.
        Accordingly, we define the cutting vector
        $cw_x = (sw_0^x, sw_1^x, \ldots, sw_r^x)$,
        where $sw_i^x < sw_j^x$ implies that layer $i$ is more sensitive to perturbations than layer $j$.
%       \Dana{please describe in high level what it means sensitivity and the vanilla graident approach}
%       \Dana{same comment, this part is important because it's the novel part}
    \end{itemize}
%    Then, the optimizer is given \Dana{complete}. %The new layer-neighborhood will be the input of the verifier in the next iteration.

%Like that, we do this in iterations many times until convergence of the layer-neighborhood and the RL\@.
%We aim to reach a maximal robust layer-neighborhood at the end of the process.
