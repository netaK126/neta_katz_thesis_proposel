\documentclass[11pt]{article}
\usepackage[a4paper, portrait, margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{caption}
\usepackage[labelformat=simple]{subcaption}    %%Adding option to remove parenthesis
\renewcommand{\thesubfigure}{\normalsize Figure \thefigure. (\alph{subfigure}):}
% \usepackage{subcaption}
% \usepackage{subcaption}
%\usepackage{dirtytalk}
\usepackage{cjhebrew}
\usepackage{float}
\usepackage{eldar_report}
% Useful packages
\usepackage{amsmath}
\usepackage{amsfonts}
% \newcommand\norm[1]{\lVert#1\rVert}
\newcommand\normx[1]{\Vert#1\Vert}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{lifetime}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{comment}
\usepackage{array}
\usepackage{float}
\newcommand{\Dana}[1]{\textcolor{purple}{\bf Dana: #1}}
\newcommand{\Neta}[1]{\textcolor{purple}{\bf Neta: #1}}
\usepackage[english]{babel}
\usepackage{amsthm}
\usepackage{cleveref}
\usepackage{amsthm}
\newtheorem{definition}{Definition}
\newtheorem{claim}{Claim}


\begin{document}
\section{Problem Definition}
In this section, we define the problem we address.

\paragraph{Image classifiers}
An image classifier $N$ maps a two-dimensional image $x\in \mathbb{R}^{n \times m}$ to a score vector over the possible set of classes $C=\{1,\ldots,c_\text{max}\}$, that is:
$N: \mathbb{R}^{n \times m} \rightarrow {\mathbb{R}}^{|C|}$.
Given an input $x$, the classification $F$ assigns to $x$ is the class with the maximal score: $class(N(x))=argmax(N(x))$.

\paragraph{Perturbation}
Give an input $x$ and a perturbation vector $\epsilon'=(\epsilon_1,...,\epsilon_k)\in{\mathbb{R}}^{k}$, a perturbation is a function $f_P(x,\epsilon')$ defining how the input $x$ is perturbed. The perturbation vector $\epsilon'$ is confined in a given range $I_\epsilon$, denoted as $\epsilon'\in I_\epsilon$, where $I_\epsilon$ is a series of intervals, each bounding the possible values of each perturbation vector's entry: $\forall{k}$ $(\epsilon')_k\in (I_\epsilon)_k$.

\paragraph{Global robustness of a single network}
%In this thesis, we follow the global robustness property defined by~\cite{DECISIONBOUND}.
%Given a perturbation $P$ and its range $I_\epsilon$, a naive approach for global robustness is:
%$\forall{x}\forall{\epsilon'}\in{I_\epsilon}$  argmax($D(x)$) = argmax($f_P(x,\epsilon')$).
%However, this property is problematic because some inputs $x$ are too close to the classifier's decision boundaries and violate this property ~\cite{DECISIONBOUND}. As a result it is impossible to satisfy it for non-trivial classifier. Thus, this study focuses on inputs whose networks' confidence in the classification is high enough ~\cite{VHAGAR}:\\
This property focuses on the minimal \emph{class confidence} $\delta$ of a network $N$, for a given class $c$, such that \emph{every} input that the network classifies with a confidence over $\delta$ is robust with respect to a perturbation $P$. We next define it formally.
Given a classifier $N$, an input $x$ and a class $c\in{C}$, the class confidence of $N$ in $c$ is:
$$\mathcal{C}(x,c,N)=N(x)_{c}-max_{c'\ne c}(N(x)_{c'})$$
%For a single network, we define the property $\delta-Global Robustness$ as follows:\\
Given a classifier $N$, an input $x$, a class $c\in{C}$, a perturbation $P$, a range $I_\epsilon$ and a class confidence $\delta>0$, the classifier $N$ is $\delta$-globally robust for $c$ under $(P,I_\epsilon)$ if:\\
$$\forall{x}\forall{\epsilon'}\in{I_\epsilon}:    \mathcal{C}(x,c,N) \geq \delta \rightarrow argmax(N(x)) = argmax(N(f_P(x,\epsilon'))) $$
Intuitively, the classifier is robust with respect to $P$ for any input classified with confidence higher or equal to the minimal $\delta$. In practice, it is easier to compute the dual bound, i.e., the maximal globally non-robust bound. That is, the maximal class confidence $\delta$ that violates the constraint:
\begin{definition}[Maximal Non-Robust Bound (Vhagar)]\label{def:vhagar}
$$\max{\delta^{vhagar}}, \texttt{ subject to } \exists{x}\exists{\epsilon'}\in{I_\epsilon}: \mathcal{C}(x,c,N) \geq \delta^{vhagar} \rightarrow \mathcal{C}(f_P(x,\epsilon'),c,N)\leq 0$$
\end{definition}
\paragraph{Problem definition} 
We study the problem of \emph{proof transfer} between two similar networks. 
Specifically, we want to utilize the robustness of a single classifier in order to upper bound the minimally globally robust bound of a similar classifier. 
Technically, we upper bound the maximally globally robust bound.
%By doing so, we would be able to enhance the algorithm's scalability.
Formally, given two classifiers $N_1,N_2: \mathbb{R}^{n \times m} \rightarrow {\mathbb{R}}^{|C|}$, a class $c\in{C}$, a perturbation $P$ and its range $I_\epsilon$, our goal is to upper bound $\delta_2^\text{vhagar}$ as tightly as possibly by $\delta_1^\text{vhagar}$. To this end, our goal is to compute:
\begin{definition}[Maximal Network Difference]
\begin{equation}\label{eq:diff}
\min \delta_\text{diff}\text{ subject to } \forall x.\ |\mathcal{C}(x,c,N_2)-\mathcal{C}(x,c,N_1)|\leq \delta_\text{diff}
\end{equation}
\end{definition}
Given this value, if we prove that: 
\begin{definition}[Maximal Non-Robust Bound for Difference]
\begin{equation}\label{eq:vagdiff}
\max{\delta^\text{max\_diff}_1}, \texttt{ subject to } \exists{x}\exists{\epsilon'}\in{I_\epsilon}: \mathcal{C}(x,c,N_1) \geq \delta^\text{max\_diff}_1 \land \mathcal{C}(f_P(x,\epsilon'),c',N_1)\leq \delta_\text{diff}
\end{equation}
\end{definition}
Note that $\delta^\text{max\_diff}_1$ is an upper bound on the maximally globally robust bound of $N_1$, that is: $\delta^\text{max\_diff}_1\geq \delta^\text{vhagar}_1$ (this follows since Vhagar's second constraint is stronger: $\leq 0$). 
Let $\Delta$ be the computer precision level, then, $\delta^\text{min\_diff}_1 = \delta^\text{max\_diff}_1+\Delta$ is an upper bound on the minimally globally robust bound of $N_1$ (which is $\delta^\text{vhagar}_1+\Delta$).

Our proof transfer determines that $\delta^\text{max\_diff}_1+\delta_\text{diff}+\Delta$ is an upper bound on $\delta^\text{vhagar}_2$. Proof: For every $x$ whose confidence by $N_2$ is at least  $\delta^\text{max\_diff}_1+\delta_\text{diff}+\Delta$, by~\Cref{eq:diff}, $N_1$ classifies this input with confidence at least $\delta^\text{max\_diff}_1+\Delta$. By~\Cref{eq:vagdiff}, for every $\epsilon'\in I_\epsilon$, we have $\mathcal{C}(f_P(x,\epsilon'),c',N_1)>\delta_\text{diff}$. By~\Cref{eq:diff}, we can infer, $\mathcal{C}(f_P(x,\epsilon'),c',N_2)>0$.

\paragraph{Determining $\delta_\text{diff}$ for~\Cref{eq:vagdiff}:}
If we could compute~\Cref{eq:diff} and then verify properties for $N_1$ (with respect to~\Cref{eq:vagdiff}), this would be easy. However, this defeats the purpose of proof transfer, since first we have $N_1$, we prove properties for it, and only then we generate $N_2$. 
Thus, instead, %we wish to predict a useful upper bound on $\delta_\text{diff}$. While we could pick some large value, this would translate to higher values of $\delta_1^{opt}$, thus lowering the region of provable global robustness. Instead, 
we change Vhagar's constraint to be \emph{parametric} in $\delta_\text{diff}$. That is, we wish to find the smallest $A$ satisfying the following constraint:
\begin{equation}\label{eq:func}
\min A\text { subject to }\forall \delta_\text{diff} \forall{x}\forall{\epsilon'}\in{I_\epsilon}:    \delta_\text{diff}>0\land \mathcal{C}(x,c,N_1) \geq \delta_\text{diff} + A\rightarrow \mathcal{C}(f_P(x,\epsilon'),c',N_1)>\delta_\text{diff}
\end{equation}
This problem is feasible since a trivial solution to this constraint is assigning to $A$ the maximal confidence $N_1$ assigns for $c$ for any input. %However, this is not helpful. 
As usual, to find a value for $A$, we look at the dual problem, looking for the max $A$ violating this constraint
%can check if it satisfies the constraint by looking for a violation, that is looking for 
%\begin{equation}\label{eq:viol}
%\exists \delta_\text{diff} \exists {x}\exists{\epsilon'}\in{I_\epsilon}:   \delta_\text{diff}>0\land  \mathcal{C}(x,c,N) \geq  \delta_\text{diff} + A\land \mathcal{C}(f_P(x,\epsilon'),c',N_2)\leq \delta_\text{diff}
%\end{equation}
%We can look for the tightest bound by solving:
\begin{equation}\label{eq:ourproof}
 \max A \text{ subject to } \delta_\text{diff}>0\land \mathcal{C}(x,c,N_1) \geq \delta_\text{diff} + A\land \mathcal{C}(f_P(x,\epsilon'),c',N_1)\leq \delta_\text{diff}
 \end{equation}
  Then, if given a new network, we proved \Cref{eq:diff}, we can infer that:
  \begin{itemize}
    \item $ \delta_\text{diff} +A+\Delta$ is an upper bound on the minimally global robust bound of $N_1$ [in other words, $N_1$ is $ \delta_\text{diff} +A+\Delta$ globally robust with respect to the given perturbation].
    \item $ 2\cdot \delta_\text{diff} +A+\Delta$ is an upper bound on the minimally global robust bound of $N_2$. Proof:
    For every $x$ whose confidence is at least $2\cdot \delta_\text{diff} +A+\Delta$, by~\Cref{eq:diff}, $N_1$ classifies this input with confidence greater than $\delta_\text{diff} +A+\Delta$. By~\Cref{eq:ourproof}, for every $\epsilon'\in I_\epsilon$, we have $\mathcal{C}(f_P(x,\epsilon'),c',N_1)>\delta_\text{diff}$. By~\Cref{eq:diff}, we can infer, $\mathcal{C}(f_P(x,\epsilon'),c',N_2)>0$. 
  \end{itemize}
  To tighten $A$ for a smaller range of $\delta_\text{diff}$, we can add an upper bound on $\delta_\text{diff}$ denoted $u$ (a hyper-parameter):
  \begin{equation}\label{eq:ourproof}
 \max A \text{ subject to } \delta_\text{diff}>0\land \delta_\text{diff}<u\land \mathcal{C}(x,c,N_1) \geq \delta_\text{diff} + A\land \mathcal{C}(f_P(x,\epsilon'),c',N_1)\leq \delta_\text{diff}
 \end{equation}
 \paragraph{New Idea}
 In fact, we can compute upper and lower bounds on $\delta^\text{vhagar}_2$ by (1)~computing lower and upper bounds on  $\delta^\text{vhagar}_1$ for different confidence levels of the perturbations and (2)~after computing $\delta_\text{diff}$, looking for the suitable levels to infer bounds for $\delta^\text{vhagar}_2$.
 \begin{definition}[$\delta_p$-Perturbation Non-Robust Bound]
 Given a confidence level for the perturbation $\delta_p\in\mathbb{R}$, we define the $\delta_p$-perturbation non-robust bound as:  
\begin{equation}\label{eq:vagbound}
\max{\delta^\text{max}_{\delta_p}}, \texttt{ subject to } \exists{x}\exists{\epsilon'}\in{I_\epsilon}: \mathcal{C}(x,c,N_1) \geq \delta^\text{max\_diff}_1 \land \mathcal{C}(f_P(x,\epsilon'),c',N_1)\leq \delta_p
\end{equation}
\end{definition}

% neta is making order

\begin{claim}\label{claim:delta_u_is_bigger_delta_v}

\end{claim}





 \begin{comment}
\begin{claim}\label{claim:n1}
Given a network $N$, a class $c$, and a perturbation $(P,I_\epsilon)$,
  for every $\delta_p$, if it is smaller than $0$ then ${\delta^\text{max}_{\delta_p}}$ is a lower bound on $\delta^\text{vhagar}$, and otherwise it is an upper bound on  $\delta^\text{vhagar}$. 
\end{claim}
Proof follows since the last condition is stronger than Vhagar's condition is its negative and looser if it is positive.

 \begin{claim}\label{claim:n2}
 Given a network $N_1$, a class $c$, and a perturbation $(P,I_\epsilon)$,
 and an array of $\delta_p$-perturbation non-robust bounds $A=[\delta^1_p,\delta^2_p,\ldots,\delta_p^k]$, for values of $p$ that are negative and positive. Given a network $N_2$ such that  $\forall x.\ |\mathcal{C}(x,c,N_2)-\mathcal{C}(x,c,N_1)|\leq \delta_\text{diff}$. Let $\delta^l_p$ be the maximal value in $A$ that is smaller than $-\delta_\text{diff}$ and $\delta^u_p$ be the minimal value in $A$ that is greater than $\delta_\text{diff}$. Then, ${\delta^\text{max}_{\delta^l_p}}-\delta_\text{diff}$ is a lower bound on  $\delta^\text{vhagar}_2$ and 
 ${\delta^\text{max}_{\delta^u_p}}+\delta_\text{diff}$ is an upper bound on  $\delta^\text{vhagar}_2$.
 \end{claim}
 Proof. \begin{itemize}
   \item 
 By~\Cref{claim:n1}, ${\delta^\text{max}_{\delta^l_p}}$ is a lower bound on $\delta^\text{vhagar}_1$.
 That is, there is $x$ whose confidence is at least ${\delta^\text{max}_{\delta^l_p}}$
 and $\mathcal{C}(f_P(x,\epsilon'),c',N_2)\leq {\delta^l_p}$.
 By~\Cref{eq:diff}, $N_2$ classifies this input with confidence at least ${\delta^\text{max}_{\delta^l_p}}-\delta_\text{diff}$
 and $\mathcal{C}(f_P(x,\epsilon'),c',N_2)\leq {\delta^l_p}+\delta_\text{diff}\leq 0$.
 By~\Cref{def:vhagar}, ${\delta^\text{max}_{\delta^l_p}}-\delta_\text{diff}$ is a lower bound on $\delta^\text{vhagar}_2$.
 \item % By~\Cref{claim:n1}, ${\delta^\text{max}_{\delta^u_p}}$ is an upper bound on $\delta^\text{vhagar}_1$.
 For every $x$ such that $C(x,N_2,c')\geq {\delta^\text{max}_{\delta^u_p}} + \delta_\text{diff} +\Delta$, by~\Cref{eq:diff}, we have
  $C(x,N_1,c')\geq {\delta^\text{max}_{\delta^u_p}} +\Delta$. By~\Cref{eq:vagbound}, for every $\epsilon'\in I_\epsilon$, we have $\mathcal{C}(f_P(x,\epsilon'),c',N_1)>{\delta^\text{max}_{\delta^u_p}}$. 
 By \Cref{eq:diff}, for every $\epsilon'\in I_\epsilon$, we have $\mathcal{C}(f_P(x,\epsilon'),c',N_2)>{\delta^\text{max}_{\delta^u_p}}-\delta_\text{diff}$. 
 Since ${\delta^\text{max}_{\delta^u_p}}>\delta_\text{diff}$, 
 we can infer, $\mathcal{C}(f_P(x,\epsilon'),c',N_2)>0$. Thus, ${\delta^\text{max}_{\delta^u_p}} + \delta_\text{diff} +\Delta$ is an upper bound on the minimally globally robust bound of $N_2$ and ${\delta^\text{max}_{\delta^u_p}} + \delta_\text{diff}$ is an upper bound on  $\delta^\text{vhagar}_2$.
 \end{itemize}
 
 \end{comment}
 \paragraph{Finding $\delta_\text{diff}$:}
 We define $\delta_\text{diff}$ as :
 $$\min \delta_\text{diff}\text{ subject to } \forall x.\ |\mathcal{C}(x,c,N_1)-\mathcal{C}(x,c,N_2)|\leq \delta_\text{diff}$$
 We can also look at the dual problem:
 $$\max \delta_\text{diff}\text{ subject to } \exists x.\ |\mathcal{C}(x,c,N_1)-\mathcal{C}(x,c,N_2)|=\delta_\text{diff}$$
 This problem is still complicated to solve using Gurobi due to the absalute function. therefore we can use the following claim:
 $\max \delta_\text{diff}\text{ subject to } \exists x.\ |\mathcal{C}(x,c,N_1)-\mathcal{C}(x,c,N_2)|= \max \{|\max_x \mathcal{C}(x,c,N_1)-\mathcal{C}(x,c,N_2)|,|\min_x \mathcal{C}(x,c,N_1)-\mathcal{C}(x,c,N_2)|\}=\delta_\text{diff}$. That way we can calculate two simple problems instead, and save time.
 
 
 \paragraph{Neta's New Idea (2.3.2025):}
 Any algorithm that answers the following is good for us:
 $$ t_{\text{vaghar}_{\delta_1}}+t_{\text{vaghar}_{\delta_2}} \geq t_\text{new algorithm}$$
 Specifically I offer:
  $$ t_{\text{vaghar}_{\delta_1}} + t_{\text{vaghar}_{\delta_2}} \geq t_{{\delta_1}_u} + t_{{\delta_1}_l} + t_{\delta_{\text{diff}}}$$
 where:
 $$ {{\delta_1}_l} \leq \delta_1 \leq {{\delta_1}_u} $$
  $$ {{\delta_1}_l}-{\delta_{\text{diff}}} \leq \delta_2 \leq {{\delta_1}_u}+{\delta_{\text{diff}}} $$
 
 \begin{itemize}
  \item Finding $\delta_{\text{diff}}$ as described above + adding some more logic so we can work with a variety of networks.
  \item Finding ${{\delta_1}_u}$ - using ~\Cref{claim:n2} (running vaghar with $\delta_{\text{diff}} \geq 0$)
  \item Finding ${{\delta_1}_l}$ - using ~\Cref{claim:n2} (running vaghar with $\delta_{\text{diff}} \leq 0$) with one single modification: ${\text{timeout}}=10\ \text{sec}$. When optimizing ${{\delta_1}_l}$ we have lower and upper bound: ${{\delta_1}_l}_l \leq {{\delta_1}_l} \leq {{\delta_1}_l}_u$. after 10 seconds ${{\delta_1}_l}_l$ is tight enough. The main issue when optimizing with gurobi is finding a tight upper bound, and specifically ${{\delta_1}_l}_u$. However, I claim that ${{\delta_1}_l}_l$ is tight enough after 10 seconds. This quality is specially profitable when we are dealing with difficult perturbations (for example: when $L_\infty$ and small $\epsilon$) where the running time is very high.
  \item This is the motivation:
  $$ t_{\text{vaghar}_{\delta_1}} \approx t_{{\delta_1}_u} $$
  $$ t_{{\delta_1}_l} = 10$$ 
  This leaves us to find an algorithm where:
  $$t_{\delta_{\text{diff}}} \ll t_{\text{vaghar}_{\delta_2}} $$
 \end{itemize}

\paragraph{Formulation (15.5.2025):}
\subparagraph{delta}
$$ \text{max}\ \delta_{1}^{\delta_{\text{diff}}^{c}}\ \text{subject to} $$
\begin{align*}
\tag{a}\quad & x\in{[0,1]^d},\ \forall{k}:\ z_{0,k}^1=x_k,\ {z_{0,k}^{1,p}}=x_k\\
\tag{b}\quad & \forall{c'\ne{c}}:\ {z_{L,c}^1}-{z_{L,c'}^1}\geq\delta_{1}^{\delta_{\text{diff}}^{c}},\   \forall{c'\ne{c}}:\ {z_{L,c}^{1,p}}-{z_{L,c'}^{1,p}}\leq{\delta_{\text{diff}}^c+M\cdot{(1-\alpha_{c'})}};\ \sum_{c'\ne{c}}{\alpha}\geq{1}\\
\tag{c}\quad &  \forall{m>0,k}:\ \hat{z}_{m,k}^1=b_{m,k}^1+\sum_{k'=1}^{k_{m-1}}{w_{m,k,k'}^1\cdot{z_{m-1,k'}^1}},\ \hat{z}_{m,k}^{1,p}=b_{m,k}^{1,p}+\sum_{k'=1}^{k_{m-1}}{w_{m,k,k'}^{1,p}\cdot{z_{m-1,k'}^{1,p}}}  \\
\tag{d}\quad & z_{m,k}^{1}\geq{0},\ z_{m,k}^{1}\geq{\hat{z}_{m,k}^{1}},\ z_{m,k}^{1}\leq{u_{m,k}^{1}\cdot{a_{m,k}^{1}}},\ z_{m,k}^{1}\leq{\hat{z}_{m,k}^{1}-l_{m,k}^{1}\cdot{(1-a_{m,k}^{1})}}\\ 
\quad & z_{m,k}^{1,p}\geq{0},\ z_{m,k}^{1,p}\geq{\hat{z}_{m,k}^{1,p}},\ z_{m,k}^{1,p}\leq{u_{m,k}^{1,p}\cdot{a_{m,k}^{1,p}}},\ z_{m,k}^{1,p}\leq{\hat{z}_{m,k}^{1,p}-l_{m,k}^{1,p}\cdot{(1-a_{m,k}^{1,p})}} 
    \\
\end{align*}

\subparagraph{diff}
we use bound prorogation to find $\delta_\text{diff}^c$:
$$I_{z_{m,k}^1,z_{m,k}^2}=[\Delta_{m,k}^l,\Delta_{m,k}^u],\ \text{max}\{z_{m,k}^1,z_{m,k}^2\}\leq{\Delta_{m,k}^u},\ \text{min}\{z_{m,k}^1,z_{m,k}^2\}\geq{\Delta_{m,k}^l}$$
$$
I_{z_{m,k}^1,z_{m,k}^2}=I_{b_{m,k}^1,b_{m,k}^2}+\sum_{k'=1}^{k_{m-1}}{w_{m,k,k'}\cdot{I_{z_{m-1,k}^1,z_{m-1,k}^2}}+I_{w_{m,k,k'}^1,w_{m,k,k'}^2}\cdot{z_{m-1,k'}^1+I_{z_{m-1,k'}^1,z_{m-1,k'}^2}} }
$$
And overall we can say:

$\delta_{\text{diff}}^c\leq{\text{max}\{|\Delta_{L,c}^u-\text{max}_{c'\ne{c}}{\Delta_{L,c'}^l}|,|\Delta_{L,c}^l-\text{max}_{c'\ne{c}}{\Delta_{L,c'}^u}|\}}$
   %\Dana{old text below:}
   
%compute the minimal class confidence $\delta_1$ and $\delta_2$, where $\forall{x}\forall{\epsilon'}\in{I_{\epsilon}}$ it holds that:
\begin{comment}
$$\max{\delta_1^{opt}}, \texttt{ subject to } \exists{x}\exists{\epsilon'}\in{I_\epsilon}: \mathcal{C}(x,c,N_1) \geq \delta_1^{opt} \rightarrow \mathcal{C}(f_P(x,\epsilon'),c',N_1)\leq \frac{\delta_1^{opt}}{10} $$
after optimizing $\delta_1^{opt}$ we optimize the following:
$$\max{\delta_2^{opt}}, \texttt{ subject to }  \exists{x}:  \mathcal{C}(x,c,N_2) \geq \delta_2^{opt} \rightarrow \mathcal{C}(x,c,N_1)\leq \delta_1^{opt}$$
Which means, we look for input $x$ such that $N_2$ would classify $x$ correctly, but $N_1$ wont be able to guarantee that $f_P(x,\epsilon')$ would be classified correctly, and so is $N_2$. 
\\
Note that We demand $\frac{\delta_1}{10}$ as an upper bound instead of $0$ since we want to maximize $\delta_2^{opt}$ for networks with larger confidence difference in the future while maintaining the global robustness property, by making sure that $\delta_1^{opt} \geq \delta^{vhagar}_1$ and $\delta_2^{opt} \geq \delta^{vhagar}_2$.
\end{comment}
\begin{comment}
\input{proofNewProblemDefinition.tex}

Formally, given two classifiers $F_1,F_2: \mathbb{R}^{n \times m} \rightarrow {\mathbb{R}}^{|C|}$, a class $c\in{C}$, a perturbation $P$ and its range $I_\epsilon$, our goal is to compute the minimal class confidence $\delta$, where $\forall{x}\forall{\epsilon'}\in{I_{\epsilon}}$ it holds that:
\begin{enumerate}[nosep,nolistsep]
    \item $\mathcal{C}(x,c',F_1) \geq \delta$ %\Dana{what is D? should it be $D_1$?} NETA - You are right. there was a mistake. I fixed it.
    \item $\mathcal{C}(x,c',F_2) > 0$
    \item $\mathcal{C}(f_P(x,\epsilon'),c',F_1)> 0$
    \item $\mathcal{C}(f_P(x,\epsilon'),c',F_2> 0$
\end{enumerate} 
To put it simply, we aim for the minimal confidence score of classifier $F_1$ for class $c'$ such that another classifier $F_2$ also predicts the input $x$ as belonging to class $c'$, and both $F_1$ and $F_2$ predict correctly the perturbed input $(f_P(x,\epsilon')$ as $c'$. However, solving this problem is difficult for existing solvers, due to the for all operators. Instead, we solve the dual problem, i.e.,  the maximal class confidence $\delta'$, where there exists an input $x$ and a perturbation amount ${\epsilon'}\in{I_\epsilon}$ satisfying:%\Dana{please explain in words this definition}
\begin{enumerate}[nosep,nolistsep]
    \item $\mathcal{C}(x,c',F_1) \geq \delta'$ %\Dana{what is D? should it be $D_1$?} NETA - You are right. there was a mistake. I fixed it.
    \item $\mathcal{C}(x,c',F_2) > 0$
    \item $\mathcal{C}(f_P(x,\epsilon'),c',F_1)\leq 0$
    \item $\mathcal{C}(f_P(x,\epsilon'),c',F_2)\leq 0$
\end{enumerate} 
The goal is to maximize the confidence score of classifier $F_1$ for a specific class $c'$ such that classifier $F_2$ also predicts the input $x$ as belonging to class $c'$. At the same time, classifiers $F_1$ and $F_2$ are unable to correctly classify the input $f_P(x,\epsilon')$, where $f_P(x,\epsilon')$ is $x$ after it has been altered by the perturbation amount $\epsilon'$. In simple terms, the objective is to simultaneously align the outputs of the two classifiers on the original input while disrupting their ability to correctly classify the perturbed version. 

\end{comment}


\end{document} 